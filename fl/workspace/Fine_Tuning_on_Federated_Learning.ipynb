{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5531e64b-e5e1-4bdb-bfc6-cb9661c44c8c",
   "metadata": {},
   "source": [
    "# ライブラリーインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551f060-aeb8-401b-98fe-9a9fbe34be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1\n",
    "!pip install torchvision==0.14.1\n",
    "!pip install setuptools>=65.5.1 # not directly required, pinned by Snyk to avoid a vulnerability\n",
    "!pip install wheel>=0.38.0 # not directly required, pinned by Snyk to avoid a vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a91cb8-c1f0-4363-a9cd-99ef2d92b6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import statistics\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import glob\n",
    "\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment\n",
    "from copy import deepcopy\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "import tqdm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3c9a5-7226-4164-99c1-7a52cd98d715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"drone_Trained.pth\"#学習済みモデルファイルへのパス（相対パスでも絶対パスでもOK）\n",
    "\n",
    "# 作成されたモデルファイルをを格納するフォルダを作成およびパスを変数にセット\n",
    "!mkdir -p Custum_Model\n",
    "SAVE_PATH = 'Custum_Model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1290f-37a9-4413-9d9c-8871fcf1c4ff",
   "metadata": {},
   "source": [
    "# データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c7ff0-87ea-4550-84e3-a1876d277e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    (0, 0, 0): 0,\n",
    "    (150, 143, 9): 1,\n",
    "}\n",
    "\n",
    "def convert_rgb_to_value(target):\n",
    "  h = target.shape[0]#画像の高さの取得\n",
    "  w = target.shape[1]#画像の横幅の取得\n",
    "  target = target.permute(2,0,1).contiguous()#テンソルの形を変換(H,W,C)->(C,H,W)\n",
    "  mask = torch.empty(h, w, dtype=torch.long)#(H,W)の2次元型を用意\n",
    "  \n",
    "  for k in mapping:#マップで定義したクラスの数繰り返し検出する\n",
    "    idx = (target==torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))#targetのある画素がmappingに定義し現在対象となっている値Kと一致すればTrueを返すテンソルidxを生成(C,H,W) \n",
    "                                                                                #(RGB値しか持たないtorch.tensor(k, dtype=torch.uint8)に対してunsueezeを2回行うことで比較可能な3次元形式に変形している)\n",
    "    validx = (idx.sum(0) == 3)#validx:RGB値すべてが一致した場合True,一致しない場合Falseの2次元テンソル（H,W）\n",
    "    mask[validx] = torch.tensor(mapping[k], dtype=torch.long)#validxがTrueだった場所に現在のKのRGB値を持つvalue値をmaskに代入\n",
    "  \n",
    "  return mask\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "t_train = A.Compose([\n",
    "    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), \n",
    "    A.HorizontalFlip(), \n",
    "    A.VerticalFlip(), \n",
    "    A.GridDistortion(p=0.2), \n",
    "    A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n",
    "    A.GaussNoise()])\n",
    "\n",
    "t_val = A.Compose([\n",
    "    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), \n",
    "    A.HorizontalFlip(),\n",
    "    A.GridDistortion(p=0.2)])\n",
    "\n",
    "class DroneDataset(Dataset):\n",
    "    \n",
    "#    def __init__(self, img_path, mask_path, X, mean, std, transform=None):\n",
    "#        self.img_path = img_path\n",
    "#        self.mask_path = mask_path\n",
    "#        self.X = X\n",
    "#        self.transform = transform\n",
    "#        self.mean = mean\n",
    "#        self.std = std\n",
    "\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "#        return len(self.X)\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#        img = cv2.imread(os.path.join(self.img_path, self.X[idx] + '.jpg'))\n",
    "#        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#        mask = cv2.imread(os.path.join(self.mask_path, self.X[idx] + '.png'))\n",
    "        img, mask = self.dataset[idx]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug['image'])\n",
    "            mask = aug['mask']\n",
    "        \n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "        \n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        mask = convert_rgb_to_value(mask)        \n",
    " \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69edd2db-6581-488a-99d5-3da1ab581d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DroneDatasetInterface(DataInterface):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "        \n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor  will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        \n",
    "        self.train_set = DroneDataset(\n",
    "            self._shard_descriptor.get_dataset('train'),\n",
    "            transform=t_train\n",
    "        )\n",
    "        self.valid_set = DroneDataset(\n",
    "            self._shard_descriptor.get_dataset('val'),\n",
    "            transform=t_val\n",
    "        )\n",
    "        \n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        generator=torch.Generator()\n",
    "        generator.manual_seed(0)\n",
    "        return DataLoader(self.train_set, batch_size=self.kwargs['train_bs'], shuffle=True, generator=generator)\n",
    "\n",
    "    def get_valid_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        return DataLoader(self.valid_set, batch_size=self.kwargs['valid_bs'], shuffle=True) \n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.train_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91834bf0-51ce-484b-89d7-98b0419dd67a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fed_dataset = DroneDatasetInterface(train_bs=4, valid_bs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6aefa-ae93-4d85-b6d3-6b9ed4eb4296",
   "metadata": {},
   "source": [
    "# モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26d175-cfad-40b5-9767-43a578522350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    'mobilenet_v2', \n",
    "    encoder_weights='imagenet', \n",
    "    classes=24, \n",
    "    activation=None, \n",
    "    encoder_depth=5, \n",
    "    decoder_channels=[256, 128, 64, 32, 16])\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# 今回の学習済みモデルを使用しない場合は下記のようにモデルを定義する。クラスが2つあることに注意。「車」と「それ以外」。\n",
    "#model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=2, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de35a8f-3bff-4a88-b6e5-f2e3b9630f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe4d1e-7874-4961-9c1f-bdffefa43c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "framework_adapter = 'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin'\n",
    "model_interface = ModelInterface(model=model, optimizer=optimizer, framework_plugin=framework_adapter)\n",
    "\n",
    "# Save the initial model state\n",
    "initial_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1072539-c399-4232-901e-3e6e6c14c47b",
   "metadata": {},
   "source": [
    "# 学習と検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a757a6-003f-4817-910d-fae13e65abcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567192a4-32e1-42f9-879c-d430ef880431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "def cross_entropy(output, target):\n",
    "    \"\"\"Binary cross-entropy metric\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(input=output,target=target)\n",
    "\n",
    "#sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434a692-a045-46f4-a613-3ae36121f7f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_interface = TaskInterface()\n",
    "\n",
    "# Task interface currently supports only standalone functions.\n",
    "@task_interface.register_fl_task(model='model', data_loader='train_loader', device='device', optimizer='optimizer')     \n",
    "def train(model, train_loader, optimizer, device):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    iou_score = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    train_loader = tqdm.tqdm(train_loader, desc=\"train\")\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    #training loop\n",
    "    for image_tiles, mask_tiles in train_loader:\n",
    "        #training phase\n",
    "        #image_tiles, mask_tiles = data\n",
    "        image = image_tiles.to(device)\n",
    "        mask = mask_tiles.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward\n",
    "        output = model(image)\n",
    "        loss = F.cross_entropy(output, mask)\n",
    "\n",
    "        #evaluation metrics\n",
    "        iou_score += mIoU(output, mask)\n",
    "        accuracy += pixel_accuracy(output, mask)\n",
    "\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#        scheduler.step() \n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    #calculatio mean for each batch\n",
    "    mean_train_loss = running_loss/len(train_loader)\n",
    "    mean_train_iou = iou_score/len(train_loader)\n",
    "    mean_train_acc = accuracy/len(train_loader)\n",
    "    print(\n",
    "#        \"Round:{}/{}..\".format(e+1, epochs),  TODO\n",
    "        \"Train Loss: {:.3f}..\".format(mean_train_loss),\n",
    "        \"Train mIoU:{:.3f}..\".format(mean_train_iou),\n",
    "        \"Train Acc:{:.3f}..\".format(mean_train_acc),\n",
    "        \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "    \n",
    "    return {'train_loss': mean_train_loss, 'train_iou': mean_train_iou, 'train_acc': mean_train_acc}\n",
    "    \n",
    "@task_interface.register_fl_task(model='model', data_loader='val_loader', device='device')     \n",
    "def validate(model, val_loader, device, loss_fn=cross_entropy):\n",
    "    torch.cuda.empty_cache()\n",
    "    min_loss = np.inf\n",
    "    decrease = 1\n",
    "    not_improve=0\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    val_iou_score = 0\n",
    "    \n",
    "    val_loader = tqdm.tqdm(val_loader, desc=\"val\")\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    #validation loop\n",
    "    with torch.no_grad():\n",
    "        for image_tiles, mask_tiles in val_loader:\n",
    "            #reshape to 9 patches from single image, delete batch size\n",
    "            #image_tiles, mask_tiles = data\n",
    "            image = image_tiles.to(device)\n",
    "            mask = mask_tiles.to(device)\n",
    "            \n",
    "            output = model(image)\n",
    "            \n",
    "            #evaluation metrics\n",
    "            val_iou_score +=  mIoU(output, mask)\n",
    "            test_accuracy += pixel_accuracy(output, mask)\n",
    "            \n",
    "            #loss\n",
    "            loss = loss_fn(output, mask)                                  \n",
    "            test_loss += loss.item()\n",
    "\n",
    "\n",
    "    if min_loss > (test_loss/len(val_loader)):\n",
    "        print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n",
    "        min_loss = (test_loss/len(val_loader))\n",
    "        decrease += 1\n",
    "        if decrease % 5 == 0:\n",
    "            print('saving model...')\n",
    "            torch.save(model.state_dict(), SAVE_PATH + '/Unet-Mobilenet_v2_mIoU-{:.3f}.pth'.format(val_iou_score/len(val_loader)))\n",
    "\n",
    "\n",
    "    if (test_loss/len(val_loader)) > min_loss:\n",
    "        not_improve += 1\n",
    "        min_loss = (test_loss/len(val_loader))\n",
    "        print(f'Loss Not Decrease for {not_improve} time')\n",
    "        #if not_improve == 7:\n",
    "            #print('Loss not decrease for 7 times, Stop Training')\n",
    "            #break\n",
    "\n",
    "    #calculatio mean for each batch\n",
    "    mean_test_loss = test_loss/len(val_loader)\n",
    "    mean_val_iou = val_iou_score/len(val_loader)\n",
    "    mean_val_acc = test_accuracy/ len(val_loader)\n",
    "    print(\n",
    "#        \"Round:{}/{}..\".format(e+1, epochs), TODO\n",
    "        \"Val Loss: {:.3f}..\".format(mean_test_loss),\n",
    "        \"Val mIoU: {:.3f}..\".format(mean_val_iou),\n",
    "        \"Val Acc:{:.3f}..\".format(mean_val_acc),\n",
    "        \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "    \n",
    "    return {'test_loss': mean_test_loss, 'val_iou': mean_val_iou, 'val_acc': mean_val_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b603c8a-e278-4225-80cc-ff7a005c0f5d",
   "metadata": {},
   "source": [
    "# 連合への接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc53402-ed41-4236-9dc9-dc9b80313a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client_id = 'api'\n",
    "director_node_fqdn = 'localhost'\n",
    "\n",
    "# 1) TLS無しで接続（検証、PoC向け）\n",
    "federation = Federation(\n",
    "    client_id=client_id, \n",
    "    director_node_fqdn=director_node_fqdn, \n",
    "    director_port='50051', \n",
    "    tls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cd2cf-f8d9-4e27-ab16-d6666403a8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "experiment_name = 'done_segmentation_experiment'\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a139e9f-25a3-4565-a7e6-a9dbe2ae3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the default federated learning plan\n",
    "import openfl.native as fx\n",
    "print(fx.get_plan(fl_plan=fl_experiment.plan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07605bd-b6a3-4993-8194-0407ba103967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "fl_experiment.start(\n",
    "    model_provider=model_interface, \n",
    "    task_keeper=task_interface,\n",
    "    data_loader=fed_dataset,\n",
    "    rounds_to_train=1,\n",
    "    opt_treatment='CONTINUE_GLOBAL',\n",
    "    override_config={'network.settings.agg_port': 50002}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560cff4b-caea-4085-91cb-4a7827cef1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If user want to stop IPython session, then reconnect and check how experiment is going\n",
    "# fl_experiment.restore_experiment_state(model_interface)\n",
    "\n",
    "fl_experiment.stream_metrics(tensorboard_logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268076d-ecc7-4411-91b5-a06ec2d03647",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = fl_experiment.get_best_model()\n",
    "torch.save(best_model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de1e33-4dde-4d1e-b75f-4952ce5f24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_experiment.remove_experiment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de7cce-06ad-404b-b471-0f6648fc711e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
