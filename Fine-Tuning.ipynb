{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f78kzzYlz6aY"
   },
   "source": [
    "# Prep #1 : Download dataset\n",
    "To download a dataset onto this Notebook, do either of the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9qTo_56z-Md"
   },
   "source": [
    "## [Option1] Download once from Google Drive to your own PC and then upload to this Colab.\n",
    "Here is the link to the google drive.\n",
    "\n",
    "https://drive.google.com/file/d/1naLdgbRmBq_QhosmXJkdmG4Pi9SiIyoo/view?usp=share_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzVXlEc1z_Eg"
   },
   "source": [
    "## [Option2] Download directly from Google Drive to here\n",
    "Run the command below\n",
    "\n",
    "Ref: https://qiita.com/namakemono/items/c963e75e0af3f7eed732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atqTrYw5z9rI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1naLdgbRmBq_QhosmXJkdmG4Pi9SiIyoo\" > /dev/null\n",
    "!CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n",
    "!curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1naLdgbRmBq_QhosmXJkdmG4Pi9SiIyoo\" -o archive.zip\n",
    "!unzip -q archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip -q archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5sipm2C0hLR"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqvarUs90ifz"
   },
   "source": [
    "# Prep #2 : Doenload the pre-trained model\n",
    "To download the model onto this Notebook, do either of the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vTki3fN0mK7"
   },
   "source": [
    "## [Option1] Download once from Google Drive to your own PC and then upload to this Colab.\n",
    "Here is the link to the google drive.\n",
    "\n",
    "https://drive.google.com/file/d/14PtYuFZc-5sB2n9lLUDku8bgyEKSLZG5/view?usp=share_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqaD-xXq0rgZ"
   },
   "source": [
    "## [Option2] Download directly from Google Drive to here\n",
    "Run the command below\n",
    "\n",
    "Ref: https://qiita.com/namakemono/items/c963e75e0af3f7eed732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_5tVDl50vBG"
   },
   "outputs": [],
   "source": [
    "!curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=14PtYuFZc-5sB2n9lLUDku8bgyEKSLZG5\" > /dev/null\n",
    "!CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n",
    "!curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=14PtYuFZc-5sB2n9lLUDku8bgyEKSLZG5\" -o drone_Trained.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BAcb5S7VD9u"
   },
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYtwL15CAvv5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import statistics\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2_N848nv2wb"
   },
   "source": [
    "# 2. Set nesesssary pathes\n",
    "Specify the paths to the folder where training images are stored, model files, and the folder where output images will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zr-FYalgCec3"
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"Custum_Dataset/Car/JPEGImages\" #Path of the folder where drone images are stored (Either relative or absolute path is acceptable)\n",
    "MASK_PATH = \"Custum_Dataset/Car/SegmentationClass\" #Path of the folder where the drone image mask image (correct label image) is stored (Either relative or absolute path is acceptable)\n",
    "MODEL_PATH = \"drone_Trained.pth\" #Path to the trained model file (Either relative or absolute paths are acceptable)\n",
    "\n",
    "# Create a folder to store the created model files and set the path to the variable\n",
    "!mkdir -p Custum_Model\n",
    "SAVE_PATH = 'Custum_Model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "872VOvhlv9Zx"
   },
   "source": [
    "# 3. Define a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of image file names to be used in the study and check the number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9vlF17-DHrP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split('.')[0])\n",
    "    \n",
    "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n",
    "\n",
    "df = create_df()\n",
    "print('Total num of images: ', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, split the whole dataset to 3 parts for training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzvdpNlWDOqN"
   },
   "outputs": [],
   "source": [
    "#10% of all data randomly isolated as test data\n",
    "X_trainval, X_test = train_test_split(df['id'].values, test_size=0.1, random_state=19)\n",
    "#15% of the rest data is randomly separated as validation data and the rest as training data\n",
    "X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n",
    "\n",
    "print('Train Size   : ', len(X_train))\n",
    "print('Val Size     : ', len(X_val))\n",
    "print('Test Size    : ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfNstf3uXbsE"
   },
   "source": [
    "Define usual Dataset and Data transformation in PyTorch way.\n",
    "In addition, here we need to implement the method called \"convert_rgb_to_value\" in terms of converting the original 3D mask images to 2D gray-scaled images, because the output shape of the model is 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWwUMiblMQUR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "t_train = A.Compose([\n",
    "    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), \n",
    "    A.HorizontalFlip(), \n",
    "    A.VerticalFlip(), \n",
    "    A.GridDistortion(p=0.2), \n",
    "    A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n",
    "    A.GaussNoise()])\n",
    "\n",
    "t_val = A.Compose([\n",
    "    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), \n",
    "    A.HorizontalFlip(),\n",
    "    A.GridDistortion(p=0.2)])\n",
    "\n",
    "# Definition of mapping between the color and class in annotation data\n",
    "mapping = {\n",
    "    (0, 0, 0): 0,\n",
    "    (150, 143, 9): 1,\n",
    "}\n",
    "\n",
    "# Convert 3D annotation data to 2D data (convert from RGB values defined in mapping to value values)\n",
    "def convert_rgb_to_value(target):\n",
    "    h = target.shape[0]#画像の高さの取得\n",
    "    w = target.shape[1]#画像の横幅の取得\n",
    "    target = target.permute(2,0,1).contiguous()#テンソルの形を変換(H,W,C)->(C,H,W)\n",
    "    mask = torch.empty(h, w, dtype=torch.long)#(H,W)の2次元型を用意\n",
    "  \n",
    "    for k in mapping:#マップで定義したクラスの数繰り返し検出する\n",
    "        idx = (target==torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))#targetのある画素がmappingに定義し現在対象となっている値Kと一致すればTrueを返すテンソルidxを生成(C,H,W) \n",
    "                                                                                #(RGB値しか持たないtorch.tensor(k, dtype=torch.uint8)に対してunsueezeを2回行うことで比較可能な3次元形式に変形している)\n",
    "        validx = (idx.sum(0) == 3)#validx:RGB値すべてが一致した場合True,一致しない場合Falseの2次元テンソル（H,W）\n",
    "        mask[validx] = torch.tensor(mapping[k], dtype=torch.long)#validxがTrueだった場所に現在のKのRGB値を持つvalue値をmaskに代入\n",
    "  \n",
    "    return mask\n",
    "\n",
    "class DroneDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_path, mask_path, X, mean, std, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(os.path.join(self.img_path, self.X[idx] + '.jpg'))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(os.path.join(self.mask_path, self.X[idx] + '.png'))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug['image'])\n",
    "            mask = aug['mask']\n",
    "        \n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "        \n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        mask = convert_rgb_to_value(mask)        \n",
    " \n",
    "        return img, mask\n",
    "\n",
    "train_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train)\n",
    "val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAeQQsMStRfa"
   },
   "source": [
    "Then, DataLoader for both training and validation is created as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMGoHWibMXlc"
   },
   "outputs": [],
   "source": [
    "batch_size= 4 \n",
    "\n",
    "generator=torch.Generator()\n",
    "generator.manual_seed(0)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LflCyxAuwWkc"
   },
   "source": [
    "# 4. Define a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTDbIDodXnoM"
   },
   "source": [
    "Loading the pre-trained model.\n",
    "\n",
    "Note: Although we only want to recognize two classes, \"cars\" and \"other,\" the base pre-trained model has been trained to classify 24 classes, so the number of classes is set to 24 to match the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQsxptkWH7IQ"
   },
   "outputs": [],
   "source": [
    "model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=24, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# If you do not use this pre-trained model, define the model as follows. Note that there are two classes. Cars\" and \"Other.\n",
    "#model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=2, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define training and validation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7b69XvKtpih"
   },
   "source": [
    "Define methods to calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOk0iz2vteQD"
   },
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaF73augt5pn"
   },
   "source": [
    "Define the method to get current learning rate from a optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSfqQXyrtxBq"
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAbmM70kuJjC"
   },
   "source": [
    "Define the fit function to train and validate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94IHIBIvuCC7"
   },
   "outputs": [],
   "source": [
    "def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_iou = []; val_acc = []\n",
    "    train_iou = []; train_acc = []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1 ; not_improve=0\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        #training loop\n",
    "        model.train()\n",
    "        \n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            #training phase\n",
    "            image_tiles, mask_tiles = data\n",
    "            \n",
    "            image = image_tiles.to(device); mask = mask_tiles.to(device);\n",
    "\n",
    "            #forward\n",
    "            output = model(image)\n",
    "            loss = criterion(output, mask)\n",
    "\n",
    "            #evaluation metrics\n",
    "            iou_score += mIoU(output, mask)\n",
    "            accuracy += pixel_accuracy(output, mask)\n",
    "\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step() #update weight img = Image.fromarray(img)\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            scheduler.step() \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        else:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            val_iou_score = 0\n",
    "            #validation loop\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    #reshape to 9 patches from single image, delete batch size\n",
    "                    image_tiles, mask_tiles = data\n",
    "                    \n",
    "                    image = image_tiles.to(device); mask = mask_tiles.to(device);\n",
    "                    output = model(image)\n",
    "                    #evaluation metrics\n",
    "                    val_iou_score +=  mIoU(output, mask)\n",
    "                    test_accuracy += pixel_accuracy(output, mask)\n",
    "                    #loss\n",
    "                    loss = criterion(output, mask)                                  \n",
    "                    test_loss += loss.item()\n",
    "            \n",
    "            #calculatio mean for each batch\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(val_loader))\n",
    "\n",
    "\n",
    "            if min_loss > (test_loss/len(val_loader)):\n",
    "                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n",
    "                min_loss = (test_loss/len(val_loader))\n",
    "                decrease += 1\n",
    "                if decrease % 5 == 0:\n",
    "                    print('saving model...')\n",
    "                    torch.save(model.state_dict(), SAVE_PATH + '/Unet-Mobilenet_v2_mIoU-{:.3f}.pth'.format(val_iou_score/len(val_loader)))\n",
    "                    \n",
    "\n",
    "            if (test_loss/len(val_loader)) > min_loss:\n",
    "                not_improve += 1\n",
    "                min_loss = (test_loss/len(val_loader))\n",
    "                print(f'Loss Not Decrease for {not_improve} time')\n",
    "                #if not_improve == 7:\n",
    "                    #print('Loss not decrease for 7 times, Stop Training')\n",
    "                    #break\n",
    "            \n",
    "            #iou\n",
    "            val_iou.append(val_iou_score/len(val_loader))\n",
    "            train_iou.append(iou_score/len(train_loader))\n",
    "            train_acc.append(accuracy/len(train_loader))\n",
    "            val_acc.append(test_accuracy/ len(val_loader))\n",
    "            print(\"Epoch:{}/{}..\".format(e+1, epochs),\n",
    "                  \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n",
    "                  \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n",
    "                  \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n",
    "                  \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n",
    "                  \"Train Acc:{:.3f}..\".format(accuracy/len(train_loader)),\n",
    "                  \"Val Acc:{:.3f}..\".format(test_accuracy/len(val_loader)),\n",
    "                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "        \n",
    "    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n",
    "               'train_miou' :train_iou, 'val_miou':val_iou,\n",
    "               'train_acc' :train_acc, 'val_acc':val_acc,\n",
    "               'lrs': lrs}\n",
    "    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5gOk8zavNb9"
   },
   "source": [
    "# 6. Run the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS9aL-7Eusi-"
   },
   "source": [
    "Define the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEiS-V5vuoR2"
   },
   "outputs": [],
   "source": [
    "max_lr = 1e-3\n",
    "epoch = 1\n",
    "weight_decay = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n",
    "                                            steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQOJ55zxvBDu"
   },
   "outputs": [],
   "source": [
    "history = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), SAVE_PATH + '/drone_Custum_Trained.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwG_dLd3C3ut"
   },
   "source": [
    "---\n",
    "## Note: This cell is specially processed for this lecture.\n",
    "Originally, we would like to run several hundred epochs of model training, but this is difficult due to time constraints, so we will replace the model with one that has already been trained for several dozen epochs and proceed with the process after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5tzjgZbCaf1"
   },
   "outputs": [],
   "source": [
    "!curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=1JXPHg4brau1T93z79VNr4VqLeCEx2CcW\" > /dev/null\n",
    "!CODE=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"\n",
    "!curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${CODE}&id=1JXPHg4brau1T93z79VNr4VqLeCEx2CcW\" -o drone_Custum_Trained.pth\n",
    "model.load_state_dict(torch.load('drone_Custum_Trained.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDeAvbp3Ddg2"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha8gGHiq3CwV"
   },
   "source": [
    "# 7. Measure the accuracy of trained model with test image set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulXGHNohxCXy"
   },
   "outputs": [],
   "source": [
    "reverse_mapping = {1: (150, 143, 9),\n",
    "                   0: (0, 0, 0),\n",
    "}      \n",
    "\n",
    "def visualize(temp):\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0,len(reverse_mapping)):\n",
    "        r[temp==l]=reverse_mapping[l][0]\n",
    "        g[temp==l]=reverse_mapping[l][1]\n",
    "        b[temp==l]=reverse_mapping[l][2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[1], temp.shape[2],3))\n",
    "\n",
    "    rgb[:,:,0] = (r)\n",
    "    rgb[:,:,1] = (g)\n",
    "    rgb[:,:,2] = (b)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zLsqxz973Fe"
   },
   "outputs": [],
   "source": [
    "class DroneTestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_path, mask_path, X, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(os.path.join(self.img_path, self.X[idx] + '.jpg'))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(os.path.join(self.mask_path, self.X[idx] + '.png'))\n",
    "        filename = str(self.X[idx])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug['image'])\n",
    "            mask = aug['mask']\n",
    "        \n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "        \n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        mask = convert_rgb_to_value(mask)\n",
    "        \n",
    "        return img, mask, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r66vSBV87tYZ"
   },
   "outputs": [],
   "source": [
    "t_test = A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST)\n",
    "test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xF5jzTtg3Qx5"
   },
   "outputs": [],
   "source": [
    "def predict_image_mask_miou(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image = t(image)\n",
    "    model.to(device); image=image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        \n",
    "        output = model(image)\n",
    "        score = mIoU(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, score\n",
    "\n",
    "def predict_image_mask_pixel(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image = t(image)\n",
    "    model.to(device); image=image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        \n",
    "        output = model(image)\n",
    "        acc = pixel_accuracy(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UWWQgOn3Y3h"
   },
   "outputs": [],
   "source": [
    "def miou_score(model, test_set):\n",
    "    score_iou = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask ,filename= test_set[i]\n",
    "        pred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "        pred_mask = pred_mask.cpu().numpy().copy()\n",
    "        pred_mask = pred_mask.reshape(1,pred_mask.shape[0],pred_mask.shape[1])\n",
    "        cv2.imwrite(os.path.join(SAVE_PATH,filename + \".png\"), visualize(pred_mask),[cv2.IMWRITE_PNG_COMPRESSION,9])#ここで画像を保存\n",
    "        score_iou.append(score)\n",
    "    mean_iou = statistics.mean(score_iou)\n",
    "    return mean_iou\n",
    "\n",
    "def pixel_acc(model, test_set):\n",
    "    accuracy = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask,filename = test_set[i]\n",
    "        pred_mask, acc = predict_image_mask_pixel(model, img, mask)\n",
    "        accuracy.append(acc)\n",
    "    mean_acc = statistics.mean(accuracy)\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBKhXd3J3fkG"
   },
   "outputs": [],
   "source": [
    "mob_miou = miou_score(model, test_set)\n",
    "print(\"miou: \" + str(mob_miou))\n",
    "mob_acc = pixel_acc(model, test_set)\n",
    "print(\"acc: \" + str(mob_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRQRa0jvtqub"
   },
   "source": [
    "# 8. Infer a single image randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYDV_xiuwr3V"
   },
   "outputs": [],
   "source": [
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpPndn6IwrQv"
   },
   "outputs": [],
   "source": [
    "def predict_image_mask_miou(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image = t(image)\n",
    "    model.to(device); image=image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        \n",
    "        output = model(image)\n",
    "        score = mIoU(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dinXo3fDtrh_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "t_test = A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST)\n",
    "test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)\n",
    "\n",
    "index = random.randint(0, len(test_set))\n",
    "img, mask, filename = test_set[index]\n",
    "pred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "pred_mask = pred_mask.cpu().numpy().copy()\n",
    "pred_mask = pred_mask.reshape(1,pred_mask.shape[0],pred_mask.shape[1])\n",
    "print(filename)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,16))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(visualize(pred_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
